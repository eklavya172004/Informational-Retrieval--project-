# -*- coding: utf-8 -*-
"""RAG implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qbdU6sKVLTbiVTSfobARoa5-MrQLZCzR
"""

!pip install sentence-transformers faiss-cpu

from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
import faiss

embed_model = SentenceTransformer("all-MiniLM-L6-v2")  # <- IMPORTANT CHANGE!

from google.colab import files
uploaded = files.upload()

import zipfile, os

zip_path = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall("corpus")

os.listdir("corpus")

records = []

for root, dirs, files in os.walk("corpus"):
    for filename in files:
        if filename.endswith(".txt"):
            with open(os.path.join(root, filename), "r", encoding="utf-8") as f:
                text = f.read()

            records.append({
                "title": filename.replace(".txt", ""),
                "clean_rag": text   # or classic version if needed
            })

df = pd.DataFrame(records)
df.head()

corpus = df["clean_rag"].tolist()
len(corpus)

embeddings = embed_model.encode(corpus, show_progress_bar=True)
embeddings = np.array(embeddings).astype("float32")
embeddings.shape

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
print("Indexed:", index.ntotal, "documents")

def semantic_search(query, top_k=5):
    query_vec = embed_model.encode([query]).astype("float32")
    distances, indices = index.search(query_vec, top_k)
    results = df.iloc[indices[0]].copy()
    results["distance"] = distances[0]
    return results

semantic_search("treatement for ringworm")

semantic_search("how to cure malaria")

semantic_search("problems caused by high blood sugar")

# -*- coding: utf-8 -*-
"""Enhanced RAG Implementation with LLM"""

# Install required packages
!pip install sentence-transformers faiss-cpu google-generativeai

from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
import faiss
import google.generativeai as genai
from google.colab import userdata

# Embedding model for semantic search
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# Configure Gemini API (Free tier available)
# Get your API key from: https://makersuite.google.com/app/apikey
print("="*80)
print("ðŸ”‘ GEMINI API SETUP")
print("="*80)
print("Get your FREE API key from: https://makersuite.google.com/app/apikey")
print()

# Option 1: Try to get from Colab Secrets first
try:
    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    print("âœ“ Found API key in Colab Secrets")
except:
    # Option 2: Prompt user to enter API key manually
    print("âš  API key not found in Colab Secrets")
    print("Please enter your Gemini API key below:")
    GEMINI_API_KEY = input("Enter API Key: ").strip()

# Configure Gemini with the API key
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

    # Use the latest Gemini 2.5 Flash model (fast and powerful)
    llm = genai.GenerativeModel('gemini-2.5-flash')
    print("âœ“ Gemini API configured successfully! (Using gemini-2.5-flash)")
    print("="*80 + "\n")
else:
    print("âŒ No API key provided. Please run again with a valid API key.")
    raise ValueError("Gemini API key is required")

from google.colab import files
print("Upload your corpus ZIP file:")
uploaded = files.upload()

import zipfile, os

zip_path = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall("corpus")

print(f"Extracted files: {os.listdir('corpus')}")

# Read all text files
records = []
for root, dirs, filenames in os.walk("corpus"):
    for filename in filenames:
        if filename.endswith(".txt"):
            with open(os.path.join(root, filename), "r", encoding="utf-8") as f:
                text = f.read()

            records.append({
                "title": filename.replace(".txt", ""),
                "content": text
            })

df = pd.DataFrame(records)
print(f"\nâœ“ Loaded {len(df)} documents")
print(df.head())

corpus = df["content"].tolist()
print(f"\nCreating embeddings for {len(corpus)} documents...")

embeddings = embed_model.encode(corpus, show_progress_bar=True)
embeddings = np.array(embeddings).astype("float32")

# Create FAISS index
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
print(f"âœ“ Indexed {index.ntotal} documents in FAISS")

def retrieve_documents(query, top_k=3):
    """
    Retrieve most relevant documents for a query
    """
    query_vec = embed_model.encode([query]).astype("float32")
    distances, indices = index.search(query_vec, top_k)

    results = df.iloc[indices[0]].copy()
    results["distance"] = distances[0]
    results["relevance_score"] = 1 / (1 + results["distance"])  # Convert distance to score

    return results

def create_prompt(query, retrieved_docs):
    """
    Create a prompt for the LLM with retrieved context
    """
    context = ""
    for idx, row in retrieved_docs.iterrows():
        context += f"\n--- Document: {row['title']} ---\n{row['content']}\n"

    prompt = f"""You are a helpful medical assistant. Use the following retrieved documents to answer the user's question accurately and comprehensively.

RETRIEVED CONTEXT:
{context}

USER QUESTION: {query}

INSTRUCTIONS:
- Base your answer primarily on the provided context
- If the context doesn't contain enough information, say so clearly
- Be accurate and cite which documents you're referencing when relevant
- Provide a clear, structured answer

ANSWER:"""

    return prompt

def generate_answer(query, top_k=3, show_context=False):
    """
    Complete RAG pipeline: Retrieve + Generate
    """
    print(f"ðŸ” Searching for: '{query}'\n")

    # Step 1: Retrieve relevant documents
    retrieved_docs = retrieve_documents(query, top_k)

    if show_context:
        print("ðŸ“š Retrieved Documents:")
        for idx, row in retrieved_docs.iterrows():
            print(f"  â€¢ {row['title']} (relevance: {row['relevance_score']:.3f})")
        print()

    # Step 2: Create prompt with context
    prompt = create_prompt(query, retrieved_docs)

    # Step 3: Generate answer using LLM
    try:
        response = llm.generate_content(prompt)
        answer = response.text

        print("ðŸ’¡ ANSWER:")
        print("=" * 80)
        print(answer)
        print("=" * 80)

        if show_context:
            print("\nðŸ“„ Sources:")
            for idx, row in retrieved_docs.iterrows():
                print(f"  â€¢ {row['title']}")

        return {
            "query": query,
            "answer": answer,
            "sources": retrieved_docs[["title", "relevance_score"]].to_dict('records')
        }

    except Exception as e:
        print(f"âŒ Error generating answer: {e}")
        return None

def rag_chat():
    """
    Interactive chat interface for RAG system
    """
    print("\n" + "="*80)
    print("ðŸ¤– RAG SYSTEM READY - Ask me anything!")
    print("="*80)
    print("Type 'quit' to exit, 'help' for options\n")

    while True:
        query = input("â“ Your question: ").strip()

        if query.lower() == 'quit':
            print("Goodbye! ðŸ‘‹")
            break

        if query.lower() == 'help':
            print("\nOptions:")
            print("  - Ask any question about your corpus")
            print("  - Type 'quit' to exit")
            print()
            continue

        if not query:
            continue

        print()
        generate_answer(query, top_k=3, show_context=True)
        print("\n" + "-"*80 + "\n")

print("\n" + "="*80)
print("EXAMPLE QUERIES")
print("="*80)

# Example 1
generate_answer("treatment for ringworm", top_k=3, show_context=True)
print("\n" + "-"*80 + "\n")

# Example 2
generate_answer("how to cure malaria", top_k=3, show_context=True)
print("\n" + "-"*80 + "\n")

# Example 3
generate_answer("problems caused by high blood sugar", top_k=3, show_context=True)

print("\n" + "="*80)
print("ðŸš€ STARTING INTERACTIVE RAG CHAT MODE")
print("="*80)
print("You can now ask any questions about your medical corpus!")
print("Commands:")
print("  â€¢ Type your question and press Enter")
print("  â€¢ Type 'quit' or 'exit' to stop")
print("  â€¢ Type 'help' for more options")
print("="*80 + "\n")

# Start the interactive chat
rag_chat()