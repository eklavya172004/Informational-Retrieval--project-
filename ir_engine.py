# -*- coding: utf-8 -*-
"""IR engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQW8wPOO8ycDx3SfMFhfeAi9V8h9bnQW
"""

# --------------------------------
# 1. Mount Google Drive
# --------------------------------
from google.colab import drive
drive.mount('/content/drive')

import os
import re
import zipfile
import numpy as np
import pandas as pd
from collections import defaultdict

ZIP_PATH = "/content/drive/MyDrive/wikipedia_corpus_clean.zip"

# ---------------------------
# 3. NLTK + CLASSIC CLEANING
# ---------------------------
import nltk
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def classic_clean(text):
    text = (text or "").lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(tokens)

# ----------------------
# 4. LOAD ZIP
# ----------------------
def load_zip(zip_path):
    docs = []
    with zipfile.ZipFile(zip_path, "r") as z:
        for name in z.namelist():
            if name.lower().endswith(".txt"):
                raw = z.read(name).decode("utf-8", errors="ignore")
                docs.append((os.path.splitext(os.path.basename(name))[0], raw))
    df = pd.DataFrame(docs, columns=["title", "text"])
    return df

df = load_zip(ZIP_PATH)
print(f"\n=== Loaded {len(df)} documents ===\n")

# ---------------------------
# 5. CLEAN CORPUS
# ---------------------------
df["clean"] = df["text"].apply(classic_clean)

# --------------------------------
# 6. Save Cleaned Corpus
# --------------------------------
SAVE_DIR = "/content/clean_corpus"
os.makedirs(SAVE_DIR, exist_ok=True)

for i, row in df.iterrows():
    with open(f"{SAVE_DIR}/{row['title']}.txt", "w", encoding="utf-8") as f:
        f.write(row["clean"])

print("Saved cleaned corpus to:", SAVE_DIR)

# ----------------------
# 6. TF-IDF VECTOR SPACE MODEL
# ----------------------
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
tfidf_mat = tfidf.fit_transform(df["clean"])

print("TF-IDF vocabulary size:", len(tfidf.vocabulary_))
print("TF-IDF matrix shape:", tfidf_mat.shape, "\n")

# ----------------------
# 7. BM25
# ----------------------
!pip install rank_bm25 > /dev/null
from rank_bm25 import BM25Okapi

bm25 = BM25Okapi([d.split() for d in df["clean"]])
print("BM25 corpus loaded with", len(df), "documents\n")

# ----------------------
# 8. VSM (Count Vectorizer)
# ----------------------
from sklearn.feature_extraction.text import CountVectorizer
vsm_vect = CountVectorizer()
vsm_mat = vsm_vect.fit_transform(df["clean"])

print("VSM vocabulary size:", len(vsm_vect.vocabulary_))
print("VSM matrix shape:", vsm_mat.shape, "\n")

# ----------------------
# 9. INVERTED INDEX
# ----------------------
inverted = defaultdict(set)
for doc_id, text in enumerate(df["clean"]):
    for term in text.split():
        inverted[term].add(doc_id)

print("Inverted index unique terms:", len(inverted))
print("\nSample postings:")
sample_terms = list(inverted.keys())[:10]
for t in sample_terms:
    print(f"{t} -> {sorted(list(inverted[t]))}")
print("\n")

# ----------------------
# 10. Similarity Matrix for PageRank / HITS
# ----------------------
sim = (tfidf_mat @ tfidf_mat.T).toarray()
np.fill_diagonal(sim, 0)
row_sum = sim.sum(axis=1, keepdims=True)
P = sim / (row_sum + 1e-12)

# ----------------------
# 11. PAGE RANK
# ----------------------
def pagerank(P, d=0.85, max_iter=100):
    n = len(P)
    r = np.ones(n)/n
    for _ in range(max_iter):
        r_new = d*(P.T@r) + (1-d)/n
        if np.linalg.norm(r_new-r,1) < 1e-6:
            break
        r = r_new
    return r

df["pagerank"] = pagerank(P)

print("=== TOP 10 PAGERANK DOCUMENTS ===")
print(df.sort_values("pagerank", ascending=False)[["title","pagerank"]].head(10), "\n")

# ----------------------
# 12. HITS (Hub / Authority)
# ----------------------
def hits(P, max_iter=100):
    n = len(P)
    h = np.ones(n)
    a = np.ones(n)
    for _ in range(max_iter):
        a_new = P.T @ h
        h_new = P @ a_new
        a_new /= np.linalg.norm(a_new)
        h_new /= np.linalg.norm(h_new)
        a, h = a_new, h_new
    return h, a

hub, auth = hits(P)
df["hub"] = hub
df["authority"] = auth

print("=== TOP 10 AUTHORITY SCORES ===")
print(df.sort_values("authority", ascending=False)[["title","authority"]].head(10), "\n")

print("=== TOP 10 HUB SCORES ===")
print(df.sort_values("hub", ascending=False)[["title","hub"]].head(10), "\n")

# ----------------------
# 13. KMEANS CLUSTERING
# ----------------------
from sklearn.cluster import KMeans

k = min(8, max(2, len(df)//40))
kmeans = KMeans(n_clusters=k, n_init=10)
df["cluster"] = kmeans.fit_predict(tfidf_mat)

print(f"=== CLUSTER DISTRIBUTION (k={k}) ===")
print(df["cluster"].value_counts(), "\n")

print("=== SAMPLE DOCS FROM EACH CLUSTER ===")
for cl in range(k):
    sample = df[df["cluster"] == cl].head(3)["title"].tolist()
    print(f"Cluster {cl}:", sample)
print("\n")

# ----------------------
# 14. NAIVE BAYES CLASSIFICATION (toy labels)
# ----------------------
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

X = vsm_mat
y = df.index % 2  # dummy labels

X_train, X_test, y_train, y_test = train_test_split(X, y)
nb = MultinomialNB().fit(X_train, y_train)
acc = nb.score(X_test, y_test)

print("=== NAIVE BAYES ACCURACY ===")
print("Accuracy:", acc, "\n")

preds = nb.predict(X_test[:10])
print("=== SAMPLE NB PREDICTIONS ===")
print("Pred:", preds)
print("True:", y_test[:10].tolist(), "\n")

# If clean() isnâ€™t defined because you ran cells out of order, redefine it here:
def clean(text):
    text = (text or "").lower()
    import re
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    stop_words = set(stopwords.words("english"))
    lemmatizer = WordNetLemmatizer()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    tokens = [t for t in text.split() if t not in stop_words and len(t) > 1]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(tokens)

# ----------------------
# 15. SEARCH DEMOS (TF-IDF, BM25, VSM)
# ----------------------
query = "computer science"
q_clean = clean(query)

print("\n========================================")
print("         SEARCH RESULTS")
print("========================================")

# TF-IDF
from sklearn.metrics.pairwise import cosine_similarity
q_vec = tfidf.transform([q_clean])
tfidf_scores = cosine_similarity(q_vec, tfidf_mat).flatten()

top = tfidf_scores.argsort()[::-1][:10]
print("\nTF-IDF top 10")
for i in top:
    print(f"{df.loc[i,'title']}  -> {tfidf_scores[i]:.4f}")

# BM25
bm25_scores = bm25.get_scores(q_clean.split())
top = np.argsort(bm25_scores)[::-1][:10]
print("\nBM25 top 10")
for i in top:
    print(f"{df.loc[i,'title']}  -> {bm25_scores[i]:.4f}")

# VSM
q_vec_vsm = vsm_vect.transform([q_clean])
vsm_scores = cosine_similarity(q_vec_vsm, vsm_mat).flatten()
top = vsm_scores.argsort()[::-1][:10]
print("\nVSM top 10")
for i in top:
    print(f"{df.loc[i,'title']}  -> {vsm_scores[i]:.4f}")

print("\n=== ALL RESULTS DISPLAYED ===\n")

# --------------------------------
# QUERY EXPANSION (WORDNET)
# --------------------------------
from nltk.corpus import wordnet as wn

def expand_query_wordnet(query):
    q_clean = clean(query)
    tokens = q_clean.split()

    expanded = set(tokens)

    for word in tokens:
        for syn in wn.synsets(word):
            for lemma in syn.lemmas():
                name = lemma.name().replace("_", " ")
                if name.isalpha() and len(name) > 2:
                    expanded.add(name.lower())

    return " ".join(list(expanded))

# ---------------------------------------------------------
# SUPER-CLEAN CORPUS QUERY EXPANSION (NO DRUG NAMES, NO CODES)
# ---------------------------------------------------------
from collections import Counter

def is_valid_general_word(w):
    # Reject numbers or mixed tokens
    if not w.isalpha():
        return False

    # Reject very short/long tokens
    if len(w) < 3 or len(w) > 10:
        return False

    # Reject biomedical junk patterns
    bad_suffixes = [
        "ib", "inib", "tinib", "fenib",
        "umab", "vir", "viro", "ase",
        "cept", "platin", "nib"
    ]
    for suf in bad_suffixes:
        if w.endswith(suf):
            return False

    # Reject all-caps weird tokens
    if w.isupper():
        return False

    return True


def expand_query_corpus(query, top_docs=5, top_terms=5):
    q_clean = clean(query)
    q_vec = tfidf.transform([q_clean])

    # similarity scores
    scores = cosine_similarity(q_vec, tfidf_mat).flatten()
    top_idx = scores.argsort()[::-1][:top_docs]

    # collect tokens from top docs
    doc_text = " ".join(df.loc[i, "clean"] for i in top_idx)
    tokens = doc_text.split()

    # Count term frequency across top docs
    freq = Counter(tokens)

    # Keep only general English-like words
    filtered = [w for w in tokens if is_valid_general_word(w)]

    # Keep only words that appear at least twice (removes rare drugs)
    filtered = [w for w in filtered if freq[w] >= 2]

    # Remove query words themselves
    filtered = [w for w in filtered if w not in q_clean.split()]

    # Unique terms
    uniq = list(set(filtered))

    if not uniq:
        return q_clean   # fallback: no expansion

    # Select top-N most common general words
    common_terms = Counter(filtered).most_common(top_terms)
    expansion_terms = [term for term, _ in common_terms]

    final_query = q_clean + " " + " ".join(expansion_terms)
    return final_query

# ============================================================
#               ROCCHIO RELEVANCE FEEDBACK (FIXED)
# ============================================================

def rocchio_feedback(query_vec, relevant_docs, non_relevant_docs,
                     alpha=1.0, beta=0.75, gamma=0.15):

    new_q = alpha * query_vec  # preserve original query

    # Add relevant documents
    if len(relevant_docs) > 0:
        rel_vecs = tfidf_mat[relevant_docs].mean(axis=0)
        new_q = new_q + beta * rel_vecs

    # Subtract non-relevant documents
    if len(non_relevant_docs) > 0:
        nonrel_vecs = tfidf_mat[non_relevant_docs].mean(axis=0)
        new_q = new_q - gamma * nonrel_vecs

    # â­ Convert to proper numpy array (avoids np.matrix errors)
    return np.asarray(new_q)


# ============================================================
#          INTERACTIVE IR SEARCH WITH QUERY EXPANSION
# ============================================================

def interactive_search(query, top_k=10):

    # Apply both expansions
    expanded = expand_query_wordnet(query)
    expanded = expand_query_corpus(expanded)

    print("\n===============================")
    print("       ORIGINAL QUERY:", query)
    print("       EXPANDED QUERY:", expanded)
    print("===============================\n")

    q_clean = expanded

    # -----------------------------------------------------------
    # TF-IDF
    # -----------------------------------------------------------
    q_vec = tfidf.transform([q_clean])
    tfidf_scores = cosine_similarity(q_vec, tfidf_mat).flatten()
    top_tfidf = tfidf_scores.argsort()[::-1][:top_k]

    print("ðŸ”µ TF-IDF Results:")
    for i in top_tfidf:
        print(f"{i}. {df.loc[i,'title']} (score={tfidf_scores[i]:.4f})")
    print("\n")

    # -----------------------------------------------------------
    # BM25
    # -----------------------------------------------------------
    bm25_scores = bm25.get_scores(q_clean.split())
    top_bm25 = np.argsort(bm25_scores)[::-1][:top_k]

    print("ðŸŸ¢ BM25 Results:")
    for i in top_bm25:
        print(f"{i}. {df.loc[i,'title']} (score={bm25_scores[i]:.4f})")
    print("\n")

    # -----------------------------------------------------------
    # VSM
    # -----------------------------------------------------------
    q_vec_vsm = vsm_vect.transform([q_clean])
    vsm_scores = cosine_similarity(q_vec_vsm, vsm_mat).flatten()
    top_vsm = vsm_scores.argsort()[::-1][:top_k]

    print("ðŸŸ£ VSM Results:")
    for i in top_vsm:
        print(f"{i}. {df.loc[i,'title']} (score={vsm_scores[i]:.4f})")
    print("\n")

    # -----------------------------------------------------------
    # PAGE RANK
    # -----------------------------------------------------------
    top_pr = df.sort_values("pagerank", ascending=False).head(top_k)

    print("ðŸŸ¡ Top PageRank Documents:")
    for idx, row in top_pr.iterrows():
        print(f"â€¢ {row['title']} (pagerank={row['pagerank']:.6f})")
    print("\n")

    # -----------------------------------------------------------
    # HITS
    # -----------------------------------------------------------
    print("ðŸ”´ Top Authorities:")
    top_auth = df.sort_values("authority", ascending=False).head(top_k)
    for idx, row in top_auth.iterrows():
        print(f"â€¢ {row['title']} (authority={row['authority']:.6f})")

    print("\nðŸŸ  Top Hubs:")
    top_hub = df.sort_values("hub", ascending=False).head(top_k)
    for idx, row in top_hub.iterrows():
        print(f"â€¢ {row['title']} (hub={row['hub']:.6f})")
    print("\n")

    # -----------------------------------------------------------
    # CLUSTER MEMBERSHIP
    # -----------------------------------------------------------
    tfidf_query_vec = tfidf.transform([q_clean])
    cluster = kmeans.predict(tfidf_query_vec)[0]

    print("ðŸŸ¤ Query belongs to Cluster:", cluster)
    print("Sample docs from this cluster:")
    print(df[df["cluster"] == cluster].head(5)["title"].tolist())
    print("\n")

    return q_vec   # needed for Rocchio


# ============================================================
#                   CLEAN SINGLE LOOP
# ============================================================

while True:

    query = input("\nEnter your search query (type 'quit' to exit): ").strip()
    if query.lower() in ["quit", "exit", "q"]:
        print("\nExiting IR system... Goodbye!")
        break

    # run normal search & get query vector
    q_vec = interactive_search(query)

    # Ask user if they want Rocchio feedback
    fb = input("Do you want to give relevance feedback? (y/n): ").strip().lower()

    if fb != "y":
        continue

    # let user enter doc IDs
    rel = input("Enter relevant doc IDs (comma-separated): ").strip()
    nonrel = input("Enter non-relevant doc IDs (comma-separated): ").strip()

    relevant_docs = list(map(int, rel.split(","))) if rel else []
    non_relevant_docs = list(map(int, nonrel.split(","))) if nonrel else []

    # compute improved query vector
    new_q_vec = rocchio_feedback(q_vec, relevant_docs, non_relevant_docs)

    # re-rank all documents
    new_scores = cosine_similarity(new_q_vec, tfidf_mat).flatten()
    new_top = new_scores.argsort()[::-1][:10]

    print("\nðŸ” Improved Results After Rocchio Feedback:")
    for i in new_top:
        print(f"{i}. {df.loc[i,'title']} (new score={new_scores[i]:.4f})")

# ============================================================
#              DISEASE GROUPS (MANUAL CLUSTERS)
# ============================================================

infectious = [
 "Syphilis", "Herpes simplex", "Toxoplasmosis",
 "Ebola virus disease", "Hepatitis C", "Rubella",
 "Warts", "Otitis media"
]

respiratory = [
 "Asthma", "Lung cancer", "Pneumoconiosis", "Vertigo"
]

metabolic = [
 "Metabolic syndrome", "Hyperlipidemia", "Hypertension",
 "Non-alcoholic fatty liver disease (NAFLD)", "Gout"
]

neurological = [
 "Concussion", "Huntington's disease", "Cerebral palsy", "Vertigo"
]

mental_health = [
 "Major depressive disorder", "Anxiety disorder",
 "Generalized Anxiety Disorder",
 "Attention-Deficit Hyperactivity Disorder"
]

genetic = [
 "Cystic fibrosis", "Ehlers-Danlos syndrome",
 "Hemochromatosis", "Celiac disease"
]

immune = [
 "Celiac disease", "Allergies", "Asthma"
]

gastro = [
 "Gastroesophageal Reflux Disease",
 "Non-alcoholic fatty liver disease (NAFLD)",
 "Celiac disease"
]

musculoskeletal = [
 "Gout", "Ehlers-Danlos syndrome"
]

import pandas as pd

def show_group(name, disease_list, color):
    df_group = pd.DataFrame({"Disease": disease_list})

    styler = df_group.style.set_caption(name).set_table_styles([
        {'selector': 'caption',
         'props': [('color', color),
                   ('font-size', '20px'),
                   ('font-weight', 'bold'),
                   ('text-align', 'left')]},
        {'selector': 'th.col_heading',
         'props': [('background-color', color),
                   ('color', 'white'),
                   ('font-size', '15px'),
                   ('text-align','left')]}
    ])

    # Remove index manually using CSS (works in ALL pandas versions)
    styler = styler.set_table_styles([
        {'selector': 'th.row_heading', 'props': [('display', 'none')]},
        {'selector': 'th.blank', 'props': [('display', 'none')]}
    ], overwrite=False)

    return styler

from IPython.display import display
display(show_group("ðŸ¦  Infectious Diseases", infectious, "#d9534f"))
display(show_group("ðŸ« Respiratory Diseases", respiratory, "#5bc0de"))
display(show_group("ðŸ” Metabolic Disorders", metabolic, "#f0ad4e"))
display(show_group("ðŸ§  Neurological Disorders", neurological, "#428bca"))
display(show_group("ðŸ˜” Mental Health Disorders", mental_health, "#6f42c1"))
display(show_group("ðŸ§¬ Genetic Disorders", genetic, "#5cb85c"))
display(show_group("ðŸ§ª Immune Disorders", immune, "#0275d8"))
display(show_group("ðŸ©º Gastrointestinal Disorders", gastro, "#d58512"))
display(show_group("ðŸ¦´ Musculoskeletal Disorders", musculoskeletal, "#8a6d3b"))

# =============================================================
#            DISEASE SIMILARITY GRAPH + PAGERANK
# =============================================================

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

# -------------------------------
# 1. Build cosine similarity matrix
# -------------------------------
from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(tfidf_mat)

# -------------------------------
# 2. Build graph using threshold
# -------------------------------
THRESHOLD = 0.20   # you can adjust this

G = nx.Graph()

# Add all diseases as nodes
for i, row in df.iterrows():
    G.add_node(i, label=row["title"])

# Add edges for similarity above threshold
for i in range(len(df)):
    for j in range(i+1, len(df)):
        sim = similarity_matrix[i][j]
        if sim >= THRESHOLD:
            G.add_edge(i, j, weight=sim)

print(f"\nGraph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")

# -------------------------------
# 3. Run PageRank on the graph
# -------------------------------
pr_scores = nx.pagerank(G, weight='weight')
df["graph_pagerank"] = df.index.map(pr_scores)

print("\n=== TOP 15 DISEASES BY GRAPH-BASED PAGERANK ===")
print(df.sort_values("graph_pagerank", ascending=False)[["title", "graph_pagerank"]].head(15))

# -------------------------------
# 4. Visualize the graph
# -------------------------------
plt.figure(figsize=(18, 12))

pos = nx.spring_layout(G, k=0.25, iterations=50)

# Node sizes based on PageRank
sizes = [5000 * df.loc[n, "graph_pagerank"] for n in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color='skyblue', alpha=0.9)
nx.draw_networkx_edges(G, pos, width=0.4, alpha=0.4)
nx.draw_networkx_labels(G, pos, labels=nx.get_node_attributes(G, 'label'), font_size=7)

plt.title("Disease Similarity Graph (TF-IDF Based)")
plt.axis("off")
plt.show()

# Get 3 sample disease names from each cluster for labeling
cluster_labels = {}

for cl in sorted(df["cluster"].unique()):
    names = df[df["cluster"] == cl]["title"].head(3).tolist()
    cluster_labels[cl] = ", ".join(names)

# ============================================================
#      BEGINNER-FRIENDLY KMEANS CLUSTERING VISUALIZATION
# ============================================================

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Reduce vectors to 2D for plotting
pca = PCA(n_components=2)
reduced = pca.fit_transform(tfidf_mat.toarray())

df["pca_x"] = reduced[:, 0]
df["pca_y"] = reduced[:, 1]

# Prepare colors
num_clusters = df["cluster"].nunique()
colors = plt.cm.Set2(np.linspace(0, 1, num_clusters))

plt.figure(figsize=(13, 9))

for cl in sorted(df["cluster"].unique()):
    cluster_points = df[df["cluster"] == cl]

    plt.scatter(
        cluster_points["pca_x"],
        cluster_points["pca_y"],
        color=colors[cl],
        s=80,
        alpha=0.7,
        label=f"Cluster {cl}: {cluster_labels[cl]}"
    )

# Title + axes + explanation
plt.title("Disease Dataset - KMeans Clustering (2D PCA Projection)", fontsize=16)
plt.xlabel("PCA Component 1 (reduced meaning of disease text)", fontsize=12)
plt.ylabel("PCA Component 2 (reduced meaning of disease text)", fontsize=12)

plt.legend(loc="best", fontsize=10)
plt.grid(alpha=0.3)
plt.show()

import matplotlib.pyplot as plt

# Plot top 10 TF-IDF scores
top_n = 10
top_idx = tfidf_scores.argsort()[::-1][:top_n]
titles = df.loc[top_idx, "title"].tolist()
scores = tfidf_scores[top_idx]

plt.figure(figsize=(12,6))
plt.barh(range(top_n), scores[::-1])
plt.yticks(range(top_n), titles[::-1])
plt.xlabel("TF-IDF Similarity Score")
plt.title("Top TF-IDF Search Results")
plt.show()

# Plot top 10 BM25 scores
top_n = 10
top_idx = np.argsort(bm25_scores)[::-1][:top_n]
titles = df.loc[top_idx, "title"].tolist()
scores = bm25_scores[top_idx]

plt.figure(figsize=(12,6))
plt.barh(range(top_n), scores[::-1])
plt.yticks(range(top_n), titles[::-1])
plt.xlabel("BM25 Score")
plt.title("Top BM25 Search Results")
plt.show()

# Plot top 10 VSM cosine similarities
top_n = 10
top_idx = vsm_scores.argsort()[::-1][:top_n]
titles = df.loc[top_idx, "title"].tolist()
scores = vsm_scores[top_idx]

plt.figure(figsize=(12,6))
plt.barh(range(top_n), scores[::-1])
plt.yticks(range(top_n), titles[::-1])
plt.xlabel("Cosine Similarity")
plt.title("Top VSM Search Results")
plt.show()

# Plot top 20 PageRank values
top_n = 20
pr_sorted = df.sort_values("pagerank", ascending=False).head(top_n)

plt.figure(figsize=(12,6))
plt.bar(pr_sorted["title"], pr_sorted["pagerank"])
plt.xticks(rotation=90)
plt.ylabel("PageRank Score")
plt.title("Top PageRank Documents")
plt.show()

# Plot top 20 Authorities
top_n = 20
auth_sorted = df.sort_values("authority", ascending=False).head(top_n)

plt.figure(figsize=(12,6))
plt.bar(auth_sorted["title"], auth_sorted["authority"])
plt.xticks(rotation=90)
plt.ylabel("Authority Score")
plt.title("Top Authority Scores (HITS)")
plt.show()

# Plot top 20 Hubs
top_n = 20
hub_sorted = df.sort_values("hub", ascending=False).head(top_n)

plt.figure(figsize=(12,6))
plt.bar(hub_sorted["title"], hub_sorted["hub"])
plt.xticks(rotation=90)
plt.ylabel("Hub Score")
plt.title("Top Hub Scores (HITS)")
plt.show()

cluster_counts = df["cluster"].value_counts().sort_index()

plt.figure(figsize=(8,6))
plt.bar(cluster_counts.index.astype(str), cluster_counts.values)
plt.xlabel("Cluster ID")
plt.ylabel("Number of Documents")
plt.title("Cluster Distribution (KMeans)")
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

preds = nb.predict(X_test)
cm = confusion_matrix(y_test, preds)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Naive Bayes Confusion Matrix")
plt.show()

from collections import Counter

all_tokens = " ".join(df["clean"]).split()
freq = Counter(all_tokens).most_common(20)
words, counts = zip(*freq)

plt.figure(figsize=(12,5))
plt.bar(words, counts)
plt.xticks(rotation=75)
plt.title("Top 20 Most Frequent Terms in Corpus")
plt.show()