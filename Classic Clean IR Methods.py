# -*- coding: utf-8 -*-
"""Copy of IR project start.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9iPIIVLou6WM7B9SJzL2111xkZsvfr1
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os

zip_path = list(uploaded.keys())[0]

with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall("corpus")

os.listdir("corpus")

import os
import pandas as pd

DATA_DIR = "corpus"  # update if needed
records = []

for root, dirs, files in os.walk(DATA_DIR):
    for filename in files:
        if filename.endswith(".txt"):
            file_path = os.path.join(root, filename)
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read().strip()

            title = filename.replace(".txt", "").strip()

            records.append({
                "title": title,
                "text": text,
                "source": "Wikipedia"
            })

df = pd.DataFrame(records)
df

df.to_csv("medical_corpus.csv", index=False)
print("Saved:", len(df), "documents")

from google.colab import files
files.download("medical_corpus.csv")

import re
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")
nltk.download("stopwords")

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()

# For classical IR (TF-IDF, BM25, VSM)
def classic_clean(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9 ]", " ", text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stop_words]
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)

# For RAG / embeddings / LLM
def rag_clean(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df["clean_classic"] = df["text"].apply(classic_clean)
df["clean_rag"]     = df["text"].apply(rag_clean)

df.head()

print("Total docs:", len(df))
print("\nSample titles:")
print(df["title"].head(10).tolist())

print("\nAverage length (original text):",
      df["text"].str.len().mean())

print("Average length (classic_clean):",
      df["clean_classic"].str.len().mean())

print("Average length (rag_clean):",
      df["clean_rag"].str.len().mean())

import os

os.makedirs("classic_clean_docs", exist_ok=True)
os.makedirs("rag_clean_docs", exist_ok=True)

for idx, row in df.iterrows():
    title = row["title"].replace("/", "_")  # avoid invalid filenames
    text  = row["clean_classic"]

    with open(f"classic_clean_docs/{title}.txt", "w", encoding="utf-8") as f:
        f.write(text)

for idx, row in df.iterrows():
    title = row["title"].replace("/", "_")
    text  = row["clean_rag"]

    with open(f"rag_clean_docs/{title}.txt", "w", encoding="utf-8") as f:
        f.write(text)

!zip -r classic_clean_docs.zip classic_clean_docs
!zip -r rag_clean_docs.zip rag_clean_docs

from google.colab import files
files.download("classic_clean_docs.zip")
files.download("rag_clean_docs.zip")

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df["clean_classic"])
tfidf_matrix.shape

import numpy as np

def search_tfidf(query, top_k=5):
    # preprocess query in same way
    q = query.lower()
    q = TfidfVectorizer().build_tokenizer()(q)  # basic tokenize
    q = " ".join(q)

    q_vec = tfidf.transform([q])
    scores = (tfidf_matrix @ q_vec.T).toarray().ravel()

    top_indices = np.argsort(scores)[::-1][:top_k]

    results = df.iloc[top_indices][["title", "text", "clean_rag"]].copy()
    results["score"] = scores[top_indices]

    return results

# --- FIXED TF-IDF Search (with classic_clean) ---

def search_tfidf(query, top_k=5):
    # Clean the query EXACTLY like documents
    q_clean = classic_clean(query)

    # Transform with trained vectorizer
    q_vec = tfidf.transform([q_clean])

    # Cosine similarity using dot product because TF-IDF is normalized
    scores = (tfidf_matrix @ q_vec.T).toarray().ravel()

    top_idx = scores.argsort()[::-1][:top_k]

    results = df.iloc[top_idx][["title", "text"]].copy()
    results["score"] = scores[top_idx]

    return results

search_tfidf("fever")

# --- FIXED BM25 Search (with classic_clean) ---

def search_bm25(query, top_k=5):
    q_clean = classic_clean(query)
    q_tokens = q_clean.split()

    scores = bm25.get_scores(q_tokens)
    top_idx = scores.argsort()[::-1][:top_k]

    results = df.iloc[top_idx][["title", "text"]].copy()
    results["score"] = scores[top_idx]

    return results

search_bm25("fever")

# --- FIXED Vector Space Model (with classic_clean) ---

def search_vsm(query, top_k=5):
    q_clean = classic_clean(query)
    q_vec = count_vect.transform([q_clean])

    sims = cosine_similarity(q_vec, tf_matrix).flatten()

    top_idx = sims.argsort()[::-1][:top_k]

    results = df.iloc[top_idx][["title", "text"]].copy()
    results["score"] = sims[top_idx]

    return results

search_vsm("infection")

# --- BUILD INVERTED INDEX ---

from collections import defaultdict

inverted_index = defaultdict(set)

for doc_id, text in enumerate(df["clean_classic"]):
    for term in text.split():
        inverted_index[term].add(doc_id)

print("Inverted index built! Total terms:", len(inverted_index))

# --- SEARCH USING INVERTED INDEX (with classic_clean) ---

def search_inverted_index(query):
    q_clean = classic_clean(query)
    tokens = q_clean.split()

    posting_lists = [inverted_index[t] for t in tokens if t in inverted_index]

    if not posting_lists:
        return "No matching documents."

    # AND search
    result_docs = set.intersection(*posting_lists)

    return df.iloc[list(result_docs)][["title", "text"]]

# Test
search_inverted_index("brain infection")

# --- PAGE RANK (TF-IDF similarity graph) ---

import numpy as np

# 1. Build cosine similarity graph using TF-IDF
similarity_sparse = tfidf_matrix @ tfidf_matrix.T     # still sparse

# Convert to dense matrix safely
similarity_matrix = similarity_sparse.toarray()        # FIXED

# 2. Remove self-links
np.fill_diagonal(similarity_matrix, 0)

# 3. Normalize rows to create a transition probability matrix
row_sum = similarity_matrix.sum(axis=1, keepdims=True)
P = similarity_matrix / (row_sum + 1e-10)

# 4. PageRank function
def pagerank(P, d=0.85, max_iter=100):
    n = P.shape[0]
    r = np.ones(n) / n       # initial rank vector
    for _ in range(max_iter):
        r = d * P.T @ r + (1 - d) / n
    return r

# 5. Compute PageRank scores
page_rank_scores = pagerank(P)

# 6. Attach to dataframe
df["pagerank"] = page_rank_scores

df.sort_values("pagerank", ascending=False).head(10)

# --- HITS (Hubs & Authorities) ---

def hits(P, max_iter=100):
    n = P.shape[0]
    h = np.ones(n)
    a = np.ones(n)

    for _ in range(max_iter):
        a = P.T @ h
        h = P @ a

        # Normalize
        a = a / (np.linalg.norm(a) + 1e-10)
        h = h / (np.linalg.norm(h) + 1e-10)

    return h, a

hub_scores, auth_scores = hits(P)

df["hub"] = hub_scores
df["authority"] = auth_scores

df.sort_values("authority", ascending=False).head(10)

# --- NAIVE BAYES CLASSIFICATION ---

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# Dummy labels (replace with real labels)
df["label"] = df.index % 2  # 0/1 alternating

X_train, X_test, y_train, y_test = train_test_split(
    df["clean_classic"], df["label"], test_size=0.2, random_state=42
)

nb_vect = CountVectorizer()
X_train_vec = nb_vect.fit_transform(X_train)
X_test_vec = nb_vect.transform(X_test)

nb_model = MultinomialNB()
nb_model.fit(X_train_vec, y_train)

print("Naive Bayes Accuracy:", nb_model.score(X_test_vec, y_test))

# --- K-MEANS CLUSTERING (TF-IDF) ---

from sklearn.cluster import KMeans

num_clusters = 5  # choose any K you want

kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
clusters = kmeans.fit_predict(tfidf_matrix)

df["cluster"] = clusters

df[["title", "cluster"]].head(10)

# --- EVALUATION METRICS ---

from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

# Use Naive Bayes predictions for evaluation
y_pred = nb_model.predict(X_test_vec)

print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

# ----- MAP & nDCG Functions -----

def dcg(scores):
    """Discounted Cumulative Gain"""
    return np.sum((2**np.array(scores) - 1) / np.log2(np.arange(2, len(scores)+2)))

def ndcg(scores):
    """Normalized DCG"""
    ideal = sorted(scores, reverse=True)
    return dcg(scores) / (dcg(ideal) + 1e-10)

def average_precision(relevance_list):
    """Compute AP for one query"""
    precisions = []
    rel_count = 0
    for i, rel in enumerate(relevance_list, start=1):
        if rel == 1:
            rel_count += 1
            precisions.append(rel_count / i)
    return np.mean(precisions) if precisions else 0.0

# Example relevance judgments
example = [1, 0, 1, 1, 0]

print("nDCG:", ndcg(example))
print("MAP:", average_precision(example))